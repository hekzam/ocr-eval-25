======================================================
📄 Documentation - evaluate_on_custom.py
======================================================

🎯 Objectif :
--------------
Ce script évalue les performances de 4 modèles de classification (entraînés préalablement)
sur un jeu de données personnalisé (images de chiffres manuscrits scannés).

📥 Données utilisées :
-----------------------
- Chemins des images : resources/paths_custom.txt
- Labels correspondants : resources/custom_label.txt
- Les images doivent être au format 28x28 pixels (comme MNIST).

📦 Modèles évalués :
---------------------
- SVM (Support Vector Machine)
- Régression Logistique (logreg)
- Random Forest
- k-Nearest Neighbors (kNN)
→ Tous chargés depuis le dossier : models/

🔄 Étapes principales :
------------------------
1. Chargement des images custom + labels (à l’aide de OpenCV).
2. Chargement des modèles pré-entraînés depuis les fichiers .pkl.
3. Prédictions sur les données custom.
4. Affichage de l’accuracy et du classification_report pour chaque modèle.
5. (Optionnel) Matrice de confusion disponible en commentaire.

🧪 Librairies utilisées :
--------------------------
- numpy
- opencv-python (cv2)
- scikit-learn
- joblib

📍 Résultats affichés :
------------------------
- Précision (accuracy) globale du modèle
- Rapport de classification par chiffre (precision, recall, f1-score)
- Support : nombre d’échantillons par classe
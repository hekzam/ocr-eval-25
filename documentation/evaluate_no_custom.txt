======================================================
ğŸ“„ Documentation - evaluate_on_custom.py
======================================================

ğŸ¯ Objectif :
--------------
Ce script Ã©value les performances de 4 modÃ¨les de classification (entraÃ®nÃ©s prÃ©alablement)
sur un jeu de donnÃ©es personnalisÃ© (images de chiffres manuscrits scannÃ©s).

ğŸ“¥ DonnÃ©es utilisÃ©es :
-----------------------
- Chemins des images : resources/paths_custom.txt
- Labels correspondants : resources/custom_label.txt
- Les images doivent Ãªtre au format 28x28 pixels (comme MNIST).

ğŸ“¦ ModÃ¨les Ã©valuÃ©s :
---------------------
- SVM (Support Vector Machine)
- RÃ©gression Logistique (logreg)
- Random Forest
- k-Nearest Neighbors (kNN)
â†’ Tous chargÃ©s depuis le dossier : models/

ğŸ”„ Ã‰tapes principales :
------------------------
1. Chargement des images custom + labels (Ã  lâ€™aide de OpenCV).
2. Chargement des modÃ¨les prÃ©-entraÃ®nÃ©s depuis les fichiers .pkl.
3. PrÃ©dictions sur les donnÃ©es custom.
4. Affichage de lâ€™accuracy et du classification_report pour chaque modÃ¨le.
5. (Optionnel) Matrice de confusion disponible en commentaire.

ğŸ§ª Librairies utilisÃ©es :
--------------------------
- numpy
- opencv-python (cv2)
- scikit-learn
- joblib

ğŸ“ RÃ©sultats affichÃ©s :
------------------------
- PrÃ©cision (accuracy) globale du modÃ¨le
- Rapport de classification par chiffre (precision, recall, f1-score)
- Support : nombre dâ€™Ã©chantillons par classe